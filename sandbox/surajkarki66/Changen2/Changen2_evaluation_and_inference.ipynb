{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GThGh4Pmdk9",
        "outputId": "a26cf0e2-0f4d-4c2f-c58c-ddc09f381367"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'pytorch-change-models' already exists and is not an empty directory.\n",
            "Requirement already satisfied: ever-beta in /usr/local/lib/python3.11/dist-packages (0.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from ever-beta) (1.26.4)\n",
            "Requirement already satisfied: prettytable in /usr/local/lib/python3.11/dist-packages (from ever-beta) (3.13.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from ever-beta) (11.1.0)\n",
            "Requirement already satisfied: albumentations>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from ever-beta) (1.4.20)\n",
            "Requirement already satisfied: tensorboard>=1.14 in /usr/local/lib/python3.11/dist-packages (from ever-beta) (2.18.0)\n",
            "Requirement already satisfied: tifffile in /usr/local/lib/python3.11/dist-packages (from ever-beta) (2025.1.10)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (from ever-beta) (0.25.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from ever-beta) (1.13.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from ever-beta) (3.10.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from ever-beta) (4.67.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from ever-beta) (2.2.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from albumentations>=0.4.2->ever-beta) (6.0.2)\n",
            "Requirement already satisfied: pydantic>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from albumentations>=0.4.2->ever-beta) (2.10.6)\n",
            "Requirement already satisfied: albucore==0.0.19 in /usr/local/lib/python3.11/dist-packages (from albumentations>=0.4.2->ever-beta) (0.0.19)\n",
            "Requirement already satisfied: eval-type-backport in /usr/local/lib/python3.11/dist-packages (from albumentations>=0.4.2->ever-beta) (0.2.2)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.11/dist-packages (from albumentations>=0.4.2->ever-beta) (4.11.0.86)\n",
            "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.19->albumentations>=0.4.2->ever-beta) (3.11.3)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=1.14->ever-beta) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=1.14->ever-beta) (1.70.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=1.14->ever-beta) (3.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboard>=1.14->ever-beta) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=1.14->ever-beta) (4.25.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=1.14->ever-beta) (75.1.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=1.14->ever-beta) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=1.14->ever-beta) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=1.14->ever-beta) (3.1.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->ever-beta) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->ever-beta) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->ever-beta) (4.55.7)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->ever-beta) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->ever-beta) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->ever-beta) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->ever-beta) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->ever-beta) (2025.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prettytable->ever-beta) (0.2.13)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image->ever-beta) (3.4.2)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image->ever-beta) (2.36.1)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->ever-beta) (0.4)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.0->albumentations>=0.4.2->ever-beta) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.0->albumentations>=0.4.2->ever-beta) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.0->albumentations>=0.4.2->ever-beta) (4.12.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard>=1.14->ever-beta) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Z-Zheng/pytorch-change-models.git\n",
        "!pip install ever-beta"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd pytorch-change-models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFuxYwNIvt7y",
        "outputId": "e0bb2eed-66e5-41ff-f74e-4101b249b7f7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/pytorch-change-models\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchange.models.changen2 import changestar_1x256\n",
        "\n",
        "model = changestar_1x256(backbone_type=\"vitb\", modeling_type=\"s1c1\", changen2_pretrained=\"s0\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4t-PMc2GmlSR",
        "outputId": "ab55cb4e-5169-4f0b-fe8b-1b86d200c7b3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.2 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n",
            "/usr/local/lib/python3.11/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/content/pytorch-change-models/torchange/models/changen2/_changestar_1x256.py:52: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(weights, map_location=torch.device('cpu')), strict=changen2_pretrained != 's0')\n",
            "2025-02-01 10:27:50, INFO:EVER:Load Changen2 pre-trained weight from EVER-Z/Changen2-ChangeStar1x256/s0_cstar_vitb_1x256.pth\n",
            "2025-02-01 10:27:50, INFO:EVER:architecture: changestar_1x256 | backbone: vitb | pre-trained data: Changen2-S0-1.2M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V20SIwKq9Y1t",
        "outputId": "ca71cee7-d058-4714-f115-22edbdcb4887"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChangeStar1xd(\n",
            "  (encoder): SAMEncoderFarSeg(\n",
            "    (vit): ImageEncoderViT(\n",
            "      (patch_embed): PatchEmbed(\n",
            "        (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
            "      )\n",
            "      (blocks): ModuleList(\n",
            "        (0-11): 12 x Block(\n",
            "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (attn): Attention(\n",
            "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (act): GELU(approximate='none')\n",
            "          )\n",
            "          (drop_path): Identity()\n",
            "        )\n",
            "      )\n",
            "      (neck): Sequential(\n",
            "        (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): LayerNorm2d()\n",
            "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (3): LayerNorm2d()\n",
            "      )\n",
            "    )\n",
            "    (sfp): SimpleFeaturePyramid(\n",
            "      (simfp_2): Sequential(\n",
            "        (0): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
            "        (1): LayerNorm2d()\n",
            "        (2): GELU(approximate='none')\n",
            "        (3): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
            "        (4): Sequential(\n",
            "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): LayerNorm2d()\n",
            "        )\n",
            "        (5): Sequential(\n",
            "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): LayerNorm2d()\n",
            "        )\n",
            "      )\n",
            "      (simfp_3): Sequential(\n",
            "        (0): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
            "        (1): Sequential(\n",
            "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): LayerNorm2d()\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): LayerNorm2d()\n",
            "        )\n",
            "      )\n",
            "      (simfp_4): Sequential(\n",
            "        (0): Sequential(\n",
            "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): LayerNorm2d()\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): LayerNorm2d()\n",
            "        )\n",
            "      )\n",
            "      (simfp_5): Sequential(\n",
            "        (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "        (1): Sequential(\n",
            "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): LayerNorm2d()\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): LayerNorm2d()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (farseg): FarSegMixin(\n",
            "      (fpn): FPN(\n",
            "        (fpn_inner1): ConvBlock(\n",
            "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Identity()\n",
            "          (2): Identity()\n",
            "        )\n",
            "        (fpn_layer1): ConvBlock(\n",
            "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (1): Identity()\n",
            "          (2): Identity()\n",
            "        )\n",
            "        (fpn_inner2): ConvBlock(\n",
            "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Identity()\n",
            "          (2): Identity()\n",
            "        )\n",
            "        (fpn_layer2): ConvBlock(\n",
            "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (1): Identity()\n",
            "          (2): Identity()\n",
            "        )\n",
            "        (fpn_inner3): ConvBlock(\n",
            "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Identity()\n",
            "          (2): Identity()\n",
            "        )\n",
            "        (fpn_layer3): ConvBlock(\n",
            "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (1): Identity()\n",
            "          (2): Identity()\n",
            "        )\n",
            "        (fpn_inner4): ConvBlock(\n",
            "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): Identity()\n",
            "          (2): Identity()\n",
            "        )\n",
            "        (fpn_layer4): ConvBlock(\n",
            "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (1): Identity()\n",
            "          (2): Identity()\n",
            "        )\n",
            "      )\n",
            "      (fsr): FSRelationV3(\n",
            "        (scene_encoder): ModuleList(\n",
            "          (0-3): 4 x Sequential(\n",
            "            (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (1): LayerNorm2d()\n",
            "            (2): GELU(approximate='none')\n",
            "            (3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (4): LayerNorm2d()\n",
            "            (5): GELU(approximate='none')\n",
            "          )\n",
            "        )\n",
            "        (project): ModuleList(\n",
            "          (0-3): 4 x Sequential(\n",
            "            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): LayerNorm2d()\n",
            "            (2): GELU(approximate='none')\n",
            "            (3): Dropout2d(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (content_encoders): ModuleList(\n",
            "          (0-3): 4 x Sequential(\n",
            "            (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (1): LayerNorm2d()\n",
            "            (2): GELU(approximate='none')\n",
            "          )\n",
            "        )\n",
            "        (feature_reencoders): ModuleList(\n",
            "          (0-3): 4 x Sequential(\n",
            "            (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (1): LayerNorm2d()\n",
            "            (2): GELU(approximate='none')\n",
            "          )\n",
            "        )\n",
            "        (normalizer): Sigmoid()\n",
            "      )\n",
            "      (dec): AssymetricDecoder(\n",
            "        (blocks): ModuleList(\n",
            "          (0): Sequential(\n",
            "            (0): Sequential(\n",
            "              (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "              (1): LayerNorm2d()\n",
            "              (2): GELU(approximate='none')\n",
            "              (3): Identity()\n",
            "            )\n",
            "          )\n",
            "          (1): Sequential(\n",
            "            (0): Sequential(\n",
            "              (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "              (1): LayerNorm2d()\n",
            "              (2): GELU(approximate='none')\n",
            "              (3): Bf16compatible(\n",
            "                (_inner_module): UpsamplingBilinear2d(scale_factor=2.0, mode='bilinear')\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (2): Sequential(\n",
            "            (0): Sequential(\n",
            "              (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "              (1): LayerNorm2d()\n",
            "              (2): GELU(approximate='none')\n",
            "              (3): Bf16compatible(\n",
            "                (_inner_module): UpsamplingBilinear2d(scale_factor=2.0, mode='bilinear')\n",
            "              )\n",
            "            )\n",
            "            (1): Sequential(\n",
            "              (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "              (1): LayerNorm2d()\n",
            "              (2): GELU(approximate='none')\n",
            "              (3): Bf16compatible(\n",
            "                (_inner_module): UpsamplingBilinear2d(scale_factor=2.0, mode='bilinear')\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (3): Sequential(\n",
            "            (0): Sequential(\n",
            "              (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "              (1): LayerNorm2d()\n",
            "              (2): GELU(approximate='none')\n",
            "              (3): Bf16compatible(\n",
            "                (_inner_module): UpsamplingBilinear2d(scale_factor=2.0, mode='bilinear')\n",
            "              )\n",
            "            )\n",
            "            (1): Sequential(\n",
            "              (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "              (1): LayerNorm2d()\n",
            "              (2): GELU(approximate='none')\n",
            "              (3): Bf16compatible(\n",
            "                (_inner_module): UpsamplingBilinear2d(scale_factor=2.0, mode='bilinear')\n",
            "              )\n",
            "            )\n",
            "            (2): Sequential(\n",
            "              (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "              (1): LayerNorm2d()\n",
            "              (2): GELU(approximate='none')\n",
            "              (3): Bf16compatible(\n",
            "                (_inner_module): UpsamplingBilinear2d(scale_factor=2.0, mode='bilinear')\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (head): ChangeMixinBiSupN1(\n",
            "    (conv): Sequential(\n",
            "      (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (1): LayerNorm2d()\n",
            "      (2): GELU(approximate='none')\n",
            "    )\n",
            "    (change_conv): ConvUpsampling(\n",
            "      (0): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): Bf16compatible(\n",
            "        (_inner_module): UpsamplingBilinear2d(scale_factor=4.0, mode='bilinear')\n",
            "      )\n",
            "    )\n",
            "    (semantic_conv): ConvUpsampling(\n",
            "      (0): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): Bf16compatible(\n",
            "        (_inner_module): UpsamplingBilinear2d(scale_factor=4.0, mode='bilinear')\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6a40gRmxoqM3",
        "outputId": "3204e0dd-188e-4303-9df6-acb9794c294e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Single Data Test"
      ],
      "metadata": {
        "id": "6D5uBQijwiAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "t1_path = \"/content/drive/MyDrive/Datasets/test_data/t1_rgb/0016.png\"\n",
        "t2_path = \"/content/drive/MyDrive/Datasets/test_data/t2_rgb/0016.png\"\n",
        "\n",
        "# Load the images\n",
        "t1_image = Image.open(t1_path)\n",
        "t2_image = Image.open(t2_path)\n",
        "\n",
        "\n",
        "# Convert the images to numpy arrays\n",
        "t1_array = np.array(t1_image).astype(np.float32) / 255.0\n",
        "t2_array = np.array(t2_image).astype(np.float32) / 255.0\n",
        "\n",
        "# Reorder the dimensions to (channels, height, width)\n",
        "t1_array = torch.tensor(np.transpose(t1_array, (2, 0, 1)))  # (height, width, channels) -> (channels, height, width)\n",
        "t2_array = torch.tensor(np.transpose(t2_array, (2, 0, 1)))  # (height, width, channels) -> (channels, height, width)\n",
        "\n",
        "# Verify the shape\n",
        "print(f\"t1_array shape: {t1_array.shape}\")\n",
        "print(f\"t2_array shape: {t2_array.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrvL4U7GvmAE",
        "outputId": "8ba90e0a-da6f-45c3-f7cf-87b80df0f34a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t1_array shape: torch.Size([3, 512, 512])\n",
            "t2_array shape: torch.Size([3, 512, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bi_images = torch.cat([t1_array.unsqueeze(0), t2_array.unsqueeze(0)], dim=1)  # [b, tc, h, w]\n",
        "bi_images.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmRP0NeTxaSA",
        "outputId": "d43ddc7a-5313-40e0-e78d-b274cab16d30"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 6, 512, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bi_images = bi_images.float()\n",
        "bi_images"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vl3jj5UlyDQg",
        "outputId": "5c39d376-44e6-4cb6-c76c-159371c8486a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[0.1098, 0.0941, 0.0980,  ..., 0.0745, 0.0824, 0.0667],\n",
              "          [0.1020, 0.0980, 0.0980,  ..., 0.0863, 0.0706, 0.0667],\n",
              "          [0.0706, 0.0706, 0.0706,  ..., 0.0706, 0.0706, 0.0706],\n",
              "          ...,\n",
              "          [0.2039, 0.1412, 0.0824,  ..., 0.1490, 0.1255, 0.0824],\n",
              "          [0.1647, 0.1216, 0.0706,  ..., 0.1451, 0.1216, 0.0667],\n",
              "          [0.1373, 0.0902, 0.0784,  ..., 0.1059, 0.0941, 0.0549]],\n",
              "\n",
              "         [[0.1098, 0.0941, 0.0863,  ..., 0.0745, 0.0863, 0.0627],\n",
              "          [0.0784, 0.0863, 0.0784,  ..., 0.0902, 0.0745, 0.0706],\n",
              "          [0.0784, 0.0784, 0.0784,  ..., 0.0824, 0.0784, 0.0784],\n",
              "          ...,\n",
              "          [0.1922, 0.1647, 0.1451,  ..., 0.1529, 0.1294, 0.0863],\n",
              "          [0.1765, 0.1608, 0.1412,  ..., 0.1137, 0.1098, 0.0667],\n",
              "          [0.1686, 0.1529, 0.1490,  ..., 0.0824, 0.0784, 0.0549]],\n",
              "\n",
              "         [[0.0667, 0.0549, 0.0627,  ..., 0.0471, 0.0549, 0.0392],\n",
              "          [0.0471, 0.0510, 0.0471,  ..., 0.0549, 0.0510, 0.0392],\n",
              "          [0.0431, 0.0431, 0.0510,  ..., 0.0471, 0.0392, 0.0471],\n",
              "          ...,\n",
              "          [0.1216, 0.1059, 0.0667,  ..., 0.0941, 0.0784, 0.0510],\n",
              "          [0.0980, 0.0941, 0.0510,  ..., 0.0824, 0.0667, 0.0431],\n",
              "          [0.0902, 0.0745, 0.0471,  ..., 0.0549, 0.0510, 0.0392]],\n",
              "\n",
              "         [[0.2549, 0.2353, 0.1922,  ..., 0.0824, 0.0902, 0.0706],\n",
              "          [0.2000, 0.2039, 0.1922,  ..., 0.0941, 0.0902, 0.0627],\n",
              "          [0.1843, 0.1765, 0.1647,  ..., 0.0824, 0.0627, 0.0588],\n",
              "          ...,\n",
              "          [0.2902, 0.2000, 0.1647,  ..., 0.1569, 0.1333, 0.1176],\n",
              "          [0.2314, 0.2000, 0.1804,  ..., 0.1882, 0.1294, 0.0902],\n",
              "          [0.1882, 0.1882, 0.1843,  ..., 0.1216, 0.0902, 0.0667]],\n",
              "\n",
              "         [[0.1804, 0.1843, 0.1725,  ..., 0.0941, 0.1059, 0.0824],\n",
              "          [0.1412, 0.1569, 0.1529,  ..., 0.1098, 0.1020, 0.0824],\n",
              "          [0.1412, 0.1412, 0.1490,  ..., 0.0980, 0.0980, 0.0902],\n",
              "          ...,\n",
              "          [0.2471, 0.2078, 0.1961,  ..., 0.1608, 0.1529, 0.1176],\n",
              "          [0.2078, 0.1843, 0.1961,  ..., 0.1686, 0.1333, 0.0902],\n",
              "          [0.1843, 0.1725, 0.1804,  ..., 0.1137, 0.0980, 0.0784]],\n",
              "\n",
              "         [[0.1255, 0.1333, 0.1059,  ..., 0.0588, 0.0667, 0.0510],\n",
              "          [0.0784, 0.0902, 0.0863,  ..., 0.0627, 0.0588, 0.0510],\n",
              "          [0.0667, 0.0745, 0.0549,  ..., 0.0588, 0.0510, 0.0549],\n",
              "          ...,\n",
              "          [0.1647, 0.1333, 0.1059,  ..., 0.1176, 0.1137, 0.0745],\n",
              "          [0.1373, 0.1176, 0.1216,  ..., 0.1098, 0.0902, 0.0667],\n",
              "          [0.1020, 0.1059, 0.1137,  ..., 0.0706, 0.0627, 0.0471]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.training = False\n",
        "predictions = model(bi_images)\n",
        "change_logit = predictions['change_prediction'].to(torch.float32)  # [b, 1, h, w]"
      ],
      "metadata": {
        "id": "hjWhHNyuxkbT"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "change_logit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U98bUp51yLhF",
        "outputId": "3da945ea-c42a-49a3-e103-bf53b13da144"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[ -4.3287,  -4.0369,  -3.7450,  ..., -10.0186,  -9.9824,  -9.9463],\n",
              "          [ -4.7808,  -4.6063,  -4.4318,  ..., -10.1676, -10.0980, -10.0285],\n",
              "          [ -5.2329,  -5.1758,  -5.1187,  ..., -10.3165, -10.2136, -10.1106],\n",
              "          ...,\n",
              "          [ -0.3197,  -0.6921,  -1.0645,  ..., -12.4711, -13.0433, -13.6155],\n",
              "          [  0.5101,  -0.0974,  -0.7048,  ..., -12.0322, -12.7063, -13.3805],\n",
              "          [  1.3399,   0.4974,  -0.3452,  ..., -11.5932, -12.3694, -13.1455]]]],\n",
              "       grad_fn=<UpsampleBilinear2DBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "change_logit.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGbWmIZRCDYo",
        "outputId": "e3c29d32-d262-4fd6-d890-2cf73835de88"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1, 512, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "change_prob = torch.sigmoid(change_logit)"
      ],
      "metadata": {
        "id": "_cQh2avyCmmM"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(change_prob)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYVwjjPTCqlq",
        "outputId": "2736e104-a17d-49bf-a255-d91a11f7fe1b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[1.3013e-02, 1.7347e-02, 2.3090e-02,  ..., 4.4562e-05,\n",
            "           4.6203e-05, 4.7904e-05],\n",
            "          [8.3192e-03, 9.8895e-03, 1.1753e-02,  ..., 3.8395e-05,\n",
            "           4.1160e-05, 4.4124e-05],\n",
            "          [5.3094e-03, 5.6198e-03, 5.9483e-03,  ..., 3.3081e-05,\n",
            "           3.6668e-05, 4.0643e-05],\n",
            "          ...,\n",
            "          [4.2074e-01, 3.3356e-01, 2.5645e-01,  ..., 3.8359e-06,\n",
            "           2.1645e-06, 1.2214e-06],\n",
            "          [6.2483e-01, 4.7568e-01, 3.3074e-01,  ..., 5.9497e-06,\n",
            "           3.0318e-06, 1.5450e-06],\n",
            "          [7.9248e-01, 6.2184e-01, 4.1455e-01,  ..., 9.2281e-06,\n",
            "           4.2466e-06, 1.9542e-06]]]], grad_fn=<SigmoidBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming change_prob is a 4D tensor with shape [batch_size, 1, height, width]\n",
        "# Apply a threshold to get the binary mask (threshold is commonly set to 0.5)\n",
        "binary_mask = (change_prob >= 0.5).float()  # Thresholding the probabilities at 0.5\n",
        "\n",
        "# You can convert to uint8 for display purposes (values will be 0 or 1)\n",
        "binary_mask_uint8 = (binary_mask * 255).to(torch.uint8)  # Convert to 0 or 255 for image display\n",
        "\n",
        "# Since we have a batch dimension, we can take the first image (batch[0]) and the first channel (change prediction)\n",
        "binary_mask_image = binary_mask_uint8[0, 0, :, :].cpu().numpy()  # Convert to NumPy array for plotting\n"
      ],
      "metadata": {
        "id": "EsKu2iVf0fW7"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Display the binary mask\n",
        "plt.imshow(binary_mask_image, cmap='gray')\n",
        "plt.title(\"Binary Mask\")\n",
        "plt.axis(\"off\")  # Hide the axes for cleaner display\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "SDim7Jx000GC",
        "outputId": "87cc40c2-bcec-4843-874a-924f11d4d67e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batch Evaluation"
      ],
      "metadata": {
        "id": "uCAOHwaZ7lfb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import imageio.v3 as imageio\n",
        "from sklearn.metrics import (\n",
        "    jaccard_score, f1_score, precision_score, recall_score, accuracy_score, cohen_kappa_score\n",
        ")"
      ],
      "metadata": {
        "id": "p2LuBKCi02EB"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths and model setup\n",
        "base_path = '/content/drive/MyDrive/Datasets/test_data/'\n",
        "t1_path = os.path.join(base_path, 't1_rgb')\n",
        "t2_path = os.path.join(base_path, 't2_rgb')\n",
        "gt_path = os.path.join(base_path, 'gt_grayscale')\n",
        "test_out_path = \"/content/drive/MyDrive/Datasets/test_data/\"\n",
        "metrics_out_file = os.path.join(test_out_path, \"metrics_results.json\")"
      ],
      "metadata": {
        "id": "6t5H5wrc7oDO"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation metrics function\n",
        "def evaluate_metrics(predicted, ground_truth):\n",
        "    # Flatten both arrays\n",
        "    predicted = predicted.flatten()\n",
        "    ground_truth = ground_truth.flatten()\n",
        "\n",
        "    # Normalize to binary labels if necessary (0 and 1)\n",
        "    if 255 in predicted or 255 in ground_truth:\n",
        "        predicted = (predicted / 255).astype(int)\n",
        "        ground_truth = (ground_truth / 255).astype(int)\n",
        "\n",
        "    # Calculate metrics\n",
        "    metrics = {\n",
        "        \"Jaccard Index (IoU)\": jaccard_score(ground_truth, predicted, average=\"binary\"),\n",
        "        \"F1 Score\": f1_score(ground_truth, predicted, average=\"binary\"),\n",
        "        \"Precision\": precision_score(ground_truth, predicted, average=\"binary\"),\n",
        "        \"Recall\": recall_score(ground_truth, predicted, average=\"binary\"),\n",
        "        \"Accuracy\": accuracy_score(ground_truth, predicted),\n",
        "        \"Cohen's Kappa Score\": cohen_kappa_score(ground_truth, predicted)\n",
        "    }\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "0VCsnh6-8TvS"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize variables to track the sum of each metric\n",
        "metric_sums = {\n",
        "    \"Jaccard Index (IoU)\": 0,\n",
        "    \"F1 Score\": 0,\n",
        "    \"Precision\": 0,\n",
        "    \"Recall\": 0,\n",
        "    \"Accuracy\": 0,\n",
        "    \"Cohen's Kappa Score\": 0\n",
        "}\n",
        "\n",
        "# Total number of images processed\n",
        "num_images = 0\n",
        "all_metrics = {}\n",
        "\n",
        "for filename in os.listdir(t1_path):\n",
        "    t1_image_path = os.path.join(t1_path, filename)\n",
        "    t2_image_path = os.path.join(t2_path, filename)\n",
        "    gt_image_path = os.path.join(gt_path, filename)\n",
        "\n",
        "    # Check if corresponding files exist\n",
        "    if not os.path.isfile(t2_image_path) or not os.path.isfile(gt_image_path):\n",
        "        print(f\"Missing corresponding file for {filename}\")\n",
        "        continue\n",
        "\n",
        "   # Load the images\n",
        "    t1_image = Image.open(t1_image_path)\n",
        "    t2_image = Image.open(t2_image_path)\n",
        "    gt_image = Image.open(gt_image_path)\n",
        "\n",
        "\n",
        "    # Convert the images to numpy arrays\n",
        "    t1_array = np.array(t1_image).astype(np.float32)\n",
        "    t2_array = np.array(t2_image).astype(np.float32)\n",
        "    gt_image = np.array(gt_image)\n",
        "\n",
        "    # Reorder the dimensions to (channels, height, width)\n",
        "    t1_array = torch.tensor(np.transpose(t1_array, (2, 0, 1)))  # (height, width, channels) -> (channels, height, width)\n",
        "    t2_array = torch.tensor(np.transpose(t2_array, (2, 0, 1)))  # (height, width, channels) -> (channels, height, width)\n",
        "\n",
        "    bi_images = torch.cat([t1_array.unsqueeze(0), t2_array.unsqueeze(0)], dim=1)\n",
        "    bi_images = bi_images.float()\n",
        "\n",
        "    # Model prediction\n",
        "    predictions = model(bi_images)\n",
        "    change_logit = predictions['change_prediction'].to(torch.float32)  # [b, 1, h, w]]\n",
        "    change_prob = torch.sigmoid(change_logit)\n",
        "\n",
        "    # Assuming change_prob is a 4D tensor with shape [batch_size, 1, height, width]\n",
        "    # Apply a threshold to get the binary mask (threshold is commonly set to 0.5)\n",
        "    binary_mask = (change_prob >= 0.5).float()  # Thresholding the probabilities at 0.5\n",
        "\n",
        "    # You can convert to uint8 for display purposes (values will be 0 or 1)\n",
        "    binary_mask_uint8 = (binary_mask * 255).to(torch.uint8)  # Convert to 0 or 255 for image display\n",
        "\n",
        "    # Since we have a batch dimension, we can take the first image (batch[0]) and the first channel (change prediction)\n",
        "    binary_mask_image = binary_mask_uint8[0, 0, :, :].cpu().numpy()  # Convert to NumPy array for plotting\n",
        "\n",
        "\n",
        "    # Save the output change map for visualization\n",
        "    imageio.imwrite(os.path.join(test_out_path, f\"{filename}_change.png\"), binary_mask_image)\n",
        "\n",
        "    # Evaluate metrics\n",
        "    metrics = evaluate_metrics(binary_mask_image, gt_image)\n",
        "    print(f\"Metrics for {filename}: {metrics}\")\n",
        "\n",
        "    # Save metrics for this image\n",
        "    all_metrics[filename] = metrics\n",
        "\n",
        "    # Add metrics to the sums\n",
        "    for metric_name in metrics:\n",
        "        metric_sums[metric_name] += metrics[metric_name]\n",
        "\n",
        "    num_images += 1\n",
        "\n",
        "# Calculate the average of each metric\n",
        "if num_images > 0:\n",
        "    avg_metrics = {metric: metric_sums[metric] / num_images for metric in metric_sums}\n",
        "    print(\"\\nAverage Metrics:\")\n",
        "    for metric, avg_value in avg_metrics.items():\n",
        "        print(f\"{metric}: {avg_value:.4f}\")\n",
        "\n",
        "    # Save metrics to JSON\n",
        "    all_metrics[\"Average\"] = avg_metrics\n",
        "    with open(metrics_out_file, \"w\") as f:\n",
        "        json.dump(all_metrics, f, indent=4)\n",
        "    print(f\"Metrics saved to {metrics_out_file}\")\n",
        "else:\n",
        "    print(\"No images processed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b96mJ_Bf8bPI",
        "outputId": "07426bcd-f1b0-4e73-bde8-0e7312997d73"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics for 0139_1.png: {'Jaccard Index (IoU)': 0.009292351679771264, 'F1 Score': 0.018413597733711047, 'Precision': 0.027339642481598318, 'Recall': 0.013881473571809931, 'Accuracy': 0.9894256591796875, \"Cohen's Kappa Score\": 0.013667166798639108}\n",
            "Metrics for 0049.png: {'Jaccard Index (IoU)': 0.5435381871250202, 'F1 Score': 0.7042756592079, 'Precision': 0.765821338816541, 'Recall': 0.6518864255153637, 'Accuracy': 0.9892616271972656, \"Cohen's Kappa Score\": 0.6988431545779548}\n",
            "Metrics for 0107.png: {'Jaccard Index (IoU)': 0.18162083936324167, 'F1 Score': 0.30740967544396813, 'Precision': 0.814935064935065, 'Recall': 0.18943396226415093, 'Accuracy': 0.9956855773925781, \"Cohen's Kappa Score\": 0.30608662701639056}\n",
            "Metrics for 0016.png: {'Jaccard Index (IoU)': 0.0051963746223564955, 'F1 Score': 0.01033902380379899, 'Precision': 0.0061419797171832595, 'Recall': 0.032649962034927864, 'Accuracy': 0.968597412109375, \"Cohen's Kappa Score\": 0.0018980625076517876}\n",
            "Metrics for 0139.png: {'Jaccard Index (IoU)': 0.25851197982345525, 'F1 Score': 0.41082164328657317, 'Precision': 0.3459915611814346, 'Recall': 0.5055487053020962, 'Accuracy': 0.995513916015625, \"Cohen's Kappa Score\": 0.40864937329517714}\n",
            "Metrics for 0101.png: {'Jaccard Index (IoU)': 0.3040656080963183, 'F1 Score': 0.46633483194176834, 'Precision': 0.32456292837985495, 'Recall': 0.828022047643183, 'Accuracy': 0.7717819213867188, \"Cohen's Kappa Score\": 0.3546788524728226}\n",
            "Metrics for 0116_1.png: {'Jaccard Index (IoU)': 0.2581125226860254, 'F1 Score': 0.41031707105969245, 'Precision': 0.34830525078369906, 'Recall': 0.4991926991926992, 'Accuracy': 0.9220314025878906, \"Cohen's Kappa Score\": 0.3699867400791369}\n",
            "Metrics for 0073.png: {'Jaccard Index (IoU)': 0.5195632393084623, 'F1 Score': 0.6838323353293413, 'Precision': 0.537222156885805, 'Recall': 0.9404982499485279, 'Accuracy': 0.98388671875, \"Cohen's Kappa Score\": 0.6761956197252807}\n",
            "Metrics for 0116.png: {'Jaccard Index (IoU)': 0.09763190693809722, 'F1 Score': 0.17789553368660105, 'Precision': 0.17722473604826547, 'Recall': 0.17857142857142858, 'Accuracy': 0.9917144775390625, \"Cohen's Kappa Score\": 0.17373185041162775}\n",
            "Metrics for 0137.png: {'Jaccard Index (IoU)': 0.018552154476176433, 'F1 Score': 0.03642848212464384, 'Precision': 0.018888009389415074, 'Recall': 0.5106068990961077, 'Accuracy': 0.4414024353027344, \"Cohen's Kappa Score\": -0.003598646621260304}\n",
            "\n",
            "Average Metrics:\n",
            "Jaccard Index (IoU): 0.2196\n",
            "F1 Score: 0.3226\n",
            "Precision: 0.3366\n",
            "Recall: 0.4350\n",
            "Accuracy: 0.9049\n",
            "Cohen's Kappa Score: 0.3000\n",
            "Metrics saved to /content/drive/MyDrive/Datasets/test_data/metrics_results.json\n"
          ]
        }
      ]
    }
  ]
}